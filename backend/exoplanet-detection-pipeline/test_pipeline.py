#!/usr/bin/env python3
"""
Comprehensive verification script for the hybrid exoplanet detection pipeline.
Tests all components end-to-end with real and synthetic data.
"""

import sys
import os
import numpy as np
import pandas as pd
import torch
import tempfile
import shutil
import glob
from pathlib import Path

# Add src to path
sys.path.insert(0, 'src')

# Configuration paths
DATA_DIR = "data"
KEPLER_CSV_GLOBAL = "data/raw/all_global.csv"
KEPLER_CSV_LOCAL = "data/raw/all_local.csv"  
KOI_CSV = "data/raw/q1_q17_dr25_sup_koi_2024.07.03_19.12.12.csv"
RESID_DIR = "data/processed/residual_windows"
PIXEL_DIR = "data/processed/pixel_diffs"

def test_environment():
    """Test environment setup and dependencies."""
    print("üß™ Testing environment setup...")
    
    try:
        # Check Python version
        python_version = sys.version
        print(f"   Python: {python_version.split()[0]}")
        
        # Test core imports
        import torch
        import sklearn
        import xgboost
        import lightkurve
        import pandas
        import numpy
        import matplotlib
        
        print(f"   PyTorch: {torch.__version__}")
        print(f"   Scikit-learn: {sklearn.__version__}")
        print(f"   XGBoost: {xgboost.__version__}")
        print(f"   Lightkurve: {lightkurve.__version__}")
        
        # Check CUDA availability
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            device_name = torch.cuda.get_device_name(0)
            print(f"   CUDA: Available ({device_name})")
        else:
            print(f"   CUDA: Not available (using CPU)")
        
        print("‚úÖ Environment setup verified")
        return True
        
    except ImportError as e:
        print(f"‚ùå Missing dependency: {e}")
        print("üí° Try: pip install -r requirements.txt")
        return False
    except Exception as e:
        print(f"‚ùå Environment test error: {e}")
        return False


def test_data_paths():
    """Test data file paths and basic structure."""
    print("üß™ Testing data paths and structure...")
    
    try:
        # Check if real data files exist
        real_data_available = False
        
        if os.path.exists(KEPLER_CSV_GLOBAL) and os.path.exists(KEPLER_CSV_LOCAL):
            print(f"   ‚úÖ Found global features: {KEPLER_CSV_GLOBAL}")
            print(f"   ‚úÖ Found local features: {KEPLER_CSV_LOCAL}")
            
            # Test feature merge
            g = pd.read_csv(KEPLER_CSV_GLOBAL)
            l = pd.read_csv(KEPLER_CSV_LOCAL)
            df = g.merge(l, on="kepid", how="inner")
            
            assert "kepid" in df and len(df) > 0, "Merge failed"
            print(f"   ‚úÖ Merged features: {len(df)} rows, {len(df.columns)} columns")
            
            real_data_available = True
            
        else:
            print(f"   ‚ö†Ô∏è  Real data not found, will use synthetic data")
            print(f"      Looking for: {KEPLER_CSV_GLOBAL}")
            print(f"      Looking for: {KEPLER_CSV_LOCAL}")
        
        # Check KOI labels
        if os.path.exists(KOI_CSV):
            koi_df = pd.read_csv(KOI_CSV)
            print(f"   ‚úÖ Found KOI labels: {len(koi_df)} entries")
            real_data_available = True
        else:
            print(f"   ‚ö†Ô∏è  KOI labels not found: {KOI_CSV}")
        
        # Create processed directories
        os.makedirs(RESID_DIR, exist_ok=True)
        os.makedirs(PIXEL_DIR, exist_ok=True)
        print(f"   ‚úÖ Created processed directories")
        
        return real_data_available
        
    except Exception as e:
        print(f"‚ùå Data path test error: {e}")
        return False


def test_processed_data():
    """Test processed data files (residuals and pixel diffs)."""
    print("üß™ Testing processed data files...")
    
    try:
        # Check residual windows
        residual_paths = glob.glob(os.path.join(RESID_DIR, "*.npy"))
        print(f"   Residual windows found: {len(residual_paths)}")
        
        if residual_paths:
            # Test a few residual windows
            for i, path in enumerate(residual_paths[:3]):
                w = np.load(path)
                assert w.shape[0] == 2 and w.ndim == 2, f"Bad residual window shape: {w.shape}"
                print(f"   ‚úÖ Residual {i+1}: {w.shape} - range [{w.min():.3f}, {w.max():.3f}]")
        else:
            print("   ‚ö†Ô∏è  No residual windows found, will create synthetic ones")
        
        # Check pixel differences  
        pixel_paths = glob.glob(os.path.join(PIXEL_DIR, "*.npy"))
        print(f"   Pixel differences found: {len(pixel_paths)}")
        
        if pixel_paths:
            # Test a few pixel differences
            for i, path in enumerate(pixel_paths[:3]):
                img = np.load(path)
                assert img.ndim == 3 and img.shape[0] == 1, f"Bad pixel diff shape: {img.shape}"
                print(f"   ‚úÖ Pixel diff {i+1}: {img.shape} - range [{img.min():.3f}, {img.max():.3f}]")
        else:
            print("   ‚ö†Ô∏è  No pixel differences found, will create synthetic ones")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Processed data test error: {e}")
        return False


def create_synthetic_data():
    """Create synthetic data for testing when real data is not available."""
    print("üß™ Creating synthetic data for testing...")
    
    try:
        # Create synthetic tabular data
        n_samples = 200
        n_features = 50
        
        kepids = np.random.randint(10000000, 20000000, n_samples)
        
        # Global features
        global_features = np.random.randn(n_samples, n_features//2)
        df_global = pd.DataFrame(global_features, columns=[f'global_{i}' for i in range(n_features//2)])
        df_global['kepid'] = kepids
        
        # Local features  
        local_features = np.random.randn(n_samples, n_features//2)
        df_local = pd.DataFrame(local_features, columns=[f'local_{i}' for i in range(n_features//2)])
        df_local['kepid'] = kepids
        
        # Labels (15% confirmed)
        labels = np.random.binomial(1, 0.15, n_samples)
        df_koi = pd.DataFrame({
            'kepid': kepids,
            'koi_disposition': ['CONFIRMED' if l else 'FALSE POSITIVE' for l in labels]
        })
        
        # Save synthetic CSV files
        os.makedirs("data/raw", exist_ok=True)
        df_global.to_csv(KEPLER_CSV_GLOBAL, index=False)
        df_local.to_csv(KEPLER_CSV_LOCAL, index=False)
        df_koi.to_csv(KOI_CSV, index=False)
        
        print(f"   ‚úÖ Created synthetic tabular data: {n_samples} samples, {n_features} features")
        
        # Create synthetic processed data for first 20 KOIs
        for i, kepid in enumerate(kepids[:20]):
            # Synthetic residual window [2, 512]
            phase = np.linspace(-0.5, 0.5, 512)
            residuals = np.random.normal(0, 0.0005, 512)
            trend = np.random.normal(0, 0.0002, 512)
            
            # Add synthetic transit for confirmed exoplanets
            if labels[i] == 1:
                transit_width = np.random.uniform(0.02, 0.08)  # Phase width
                transit_depth = np.random.uniform(0.0005, 0.002)  # Depth
                in_transit = np.abs(phase) < transit_width
                residuals[in_transit] -= transit_depth
            
            residual_window = np.stack([residuals, trend], axis=0).astype(np.float32)
            np.save(os.path.join(RESID_DIR, f"residual_{kepid}.npy"), residual_window)
            
            # Synthetic pixel difference [1, 16, 16]
            pixel_diff = np.random.normal(0, 0.1, (1, 16, 16)).astype(np.float32)
            
            # Add synthetic transit signal for confirmed exoplanets
            if labels[i] == 1:
                center_y, center_x = np.random.randint(5, 11, 2)  # Random center
                radius = np.random.uniform(2, 4)
                depth = np.random.uniform(0.2, 0.8)
                
                y, x = np.ogrid[:16, :16]
                mask = (y - center_y)**2 + (x - center_x)**2 <= radius**2
                pixel_diff[0, mask] -= depth
            
            np.save(os.path.join(PIXEL_DIR, f"pixdiff_{kepid}.npy"), pixel_diff)
        
        print(f"   ‚úÖ Created synthetic processed data for 20 KOIs")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Synthetic data creation error: {e}")
        return False

def test_models():
    """Test model creation and forward pass."""
    print("üß™ Testing model architectures...")
    
    try:
        from models import create_models
        
        # Create models with correct config
        config = {
            'tabular_input_size': 50,
            'cnn1d_input_length': 512,
            'cnn2d_phases': 1,
            'cnn2d_image_size': (16, 16),
            'dropout_rate': 0.3
        }
        models = create_models(config)
        
        # Test forward pass
        batch_size = 4
        tabular_input = torch.randn(batch_size, 50)
        residual_input = torch.randn(batch_size, 2, 512)
        pixel_input = torch.randn(batch_size, 1, 16, 16)
        
        with torch.no_grad():
            tab_out = models['tabular'](tabular_input)
            res_out = models['cnn1d'](residual_input)
            pix_out = models['cnn2d'](pixel_input)
        
        # Check outputs
        assert tab_out.shape == (batch_size, 1)
        assert res_out.shape == (batch_size, 1)  
        assert pix_out.shape == (batch_size, 1)
        
        print("‚úÖ Model architectures working correctly")
        return True
        
    except Exception as e:
        print(f"‚ùå Model test error: {e}")
        return False

def test_imports():
    """Test that all required modules can be imported."""
    print("üß™ Testing module imports...")
    
    try:
        # Add src to path first
        sys.path.insert(0, 'src')
        
        import data_loader
        import features  
        import pixel_diff
        import models
        import train
        import evaluate
        print("‚úÖ All custom modules imported successfully")
        return True
    except ImportError as e:
        print(f"‚ùå Module import error: {e}")
        return False


def test_data_processing():
    """Test data loading and processing functionality."""
    print("üß™ Testing data processing functionality...")
    
    try:
        from data_loader import load_and_prepare_data, create_train_val_test_splits
        
        # Test with synthetic data
        print("   Testing data loading pipeline...")
        data = load_and_prepare_data("data/raw")
        
        if data['features']:
            print(f"   ‚úÖ Loaded {len(data['features'])} feature sets")
            
            # Test splitting
            splits = create_train_val_test_splits(data)
            
            print(f"   ‚úÖ Created data splits:")
            print(f"      Train: {len(splits['train']['y'])} samples")
            print(f"      Val: {len(splits['val']['y'])} samples") 
            print(f"      Test: {len(splits['test']['y'])} samples")
            print(f"      Features: {splits['train']['X'].shape[1]}")
            
            # Check label distribution
            for split_name in ['train', 'val', 'test']:
                labels = splits[split_name]['y']
                pos_rate = np.mean(labels)
                print(f"      {split_name} positive rate: {pos_rate:.1%}")
            
            return True
        else:
            print("   ‚ö†Ô∏è  No features loaded, but function ran successfully")
            return True
            
    except Exception as e:
        print(f"‚ùå Data processing test error: {e}")
        return False


def test_training_pipeline():
    """Test a quick training run with minimal data."""
    print("üß™ Testing quick training run...")
    
    try:
        from train import main_training_pipeline
        
        # Create minimal synthetic data if needed
        if not os.path.exists(KEPLER_CSV_GLOBAL):
            create_synthetic_data()
        
        print("   Starting minimal training run (3 epochs, 50 samples)...")
        
        # Create minimal config
        config = {
            'data_dir': 'data/raw/',
            'batch_size': 16,
            'n_epochs': 3,
            'learning_rate': 0.001,
            'models_to_train': ['tabular'],
            'save_models': False,
            'train_fusion': False,
            'use_gpu': False,
            'random_state': 42
        }
        
        results = main_training_pipeline(config)
        
        print("   ‚úÖ Training completed successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Training test error: {e}")
        return False
        
        # Check outputs
        if 'results' in results:
            for model_name, metrics in results['results'].items():
                print(f"      {model_name}: ROC-AUC={metrics['roc_auc']:.3f}, PR-AUC={metrics['pr_auc']:.3f}")
        
        # Check saved files
        model_dir = results.get('model_dir', 'models/test_run')
        if os.path.exists(model_dir):
            saved_files = os.listdir(model_dir)
            print(f"      Saved files: {len(saved_files)} files in {model_dir}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Training test error: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run comprehensive verification tests."""
    print("üöÄ Starting Hybrid Exoplanet Detection Pipeline Verification")
    print("=" * 70)
    
    # Step 1: Environment sanity check
    print("\nüìã Step 1: Environment Sanity Check")
    print("-" * 40)
    env_ok = test_environment()
    
    if not env_ok:
        print("‚ùå Environment check failed. Please install requirements first:")
        print("   pip install -r requirements.txt")
        return False
    
    # Step 2: Point the data paths  
    print("\nüìã Step 2: Data Path Configuration")
    print("-" * 40)
    real_data = test_data_paths()
    
    if not real_data:
        print("   Creating synthetic data for testing...")
        create_synthetic_data()
    
    # Step 3: Test processed data
    print("\nüìã Step 3: Processed Data Verification")
    print("-" * 40)
    test_processed_data()
    
    # Step 4: Test module imports
    print("\nüìã Step 4: Module Import Test")
    print("-" * 40)
    imports_ok = test_imports()
    
    if not imports_ok:
        print("‚ùå Module imports failed. Check your src/ directory structure.")
        return False
    
    # Step 5: Test model architectures
    print("\nüìã Step 5: Model Architecture Test")
    print("-" * 40)
    models_ok = test_models()
    
    if not models_ok:
        print("‚ùå Model architecture test failed.")
        return False
    
    # Step 6: Test data processing pipeline
    print("\nüìã Step 6: Data Processing Pipeline")
    print("-" * 40)
    data_ok = test_data_processing()
    
    if not data_ok:
        print("‚ùå Data processing test failed.")
        return False
    
    # Step 7: Quick training test (smoke test)
    print("\nüìã Step 7: Quick Training Test (Smoke Test)")
    print("-" * 40)
    training_ok = test_training_pipeline()
    
    if not training_ok:
        print("‚ùå Training test failed.")
        return False
    
    # Final summary
    print("\n" + "=" * 70)
    print("üéâ ALL VERIFICATION TESTS PASSED!")
    print("=" * 70)
    
    print("\n‚úÖ Your pipeline is ready! Next steps:")
    print("   1. üî¨ Explore the demo: jupyter notebook notebooks/demo_pipeline.ipynb")
    print("   2. üèãÔ∏è  Full training: python src/train.py --epochs 50 --batch-size 32")  
    print("   3. üìä Check results in models/ directory")
    print("   4. üöÄ Scale up with real Kepler data!")
    
    print("\nüéØ Expected Performance Targets:")
    print("   ‚Ä¢ Individual models: ROC-AUC 0.85-0.90")
    print("   ‚Ä¢ Fusion stacker: ROC-AUC 0.92-0.95")
    print("   ‚Ä¢ Recall@1%FPR: 0.85+")
    
    return True


def run_quick_check():
    """Run just the essential quick checks."""
    print("‚ö° Quick Health Check")
    print("-" * 30)
    
    try:
        # Environment
        import torch, lightkurve, sklearn, xgboost
        print("‚úÖ Dependencies OK")
        
        # CUDA
        if torch.cuda.is_available():
            print(f"‚úÖ CUDA: {torch.cuda.get_device_name(0)}")
        else:
            print("‚úÖ CUDA: Not available (using CPU)")
        
        # Data paths
        if os.path.exists(KEPLER_CSV_GLOBAL):
            print("‚úÖ Real data found")
        else:
            print("‚ö†Ô∏è  Using synthetic data")
        
        # Processed data
        residuals = len(glob.glob(os.path.join(RESID_DIR, "*.npy")))
        pixels = len(glob.glob(os.path.join(PIXEL_DIR, "*.npy")))
        print(f"‚úÖ Processed: {residuals} residuals, {pixels} pixel diffs")
        
        # Models
        sys.path.insert(0, 'src')
        from models import create_models
        config = {'tabular_input_size': 50, 'dropout_rate': 0.3}
        models = create_models(config)
        print("‚úÖ Models created successfully")
        
        print("\nüöÄ System ready for full pipeline test!")
        return True
        
    except Exception as e:
        print(f"‚ùå Quick check failed: {e}")
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Verify hybrid exoplanet detection pipeline")
    parser.add_argument("--quick", action="store_true", help="Run quick health check only")
    parser.add_argument("--full", action="store_true", help="Run full verification suite")
    
    args = parser.parse_args()
    
    if args.quick:
        success = run_quick_check()
    else:
        success = main()
    
    if success:
        print("\nüéä Pipeline verification completed successfully!")
    else:
        print("\nüí• Pipeline verification failed. Check errors above.")
        
    sys.exit(0 if success else 1)